\chapter{API specification and implementation}
\label{ch:spec}


\section{Methodology}
\label{sec:method}
The definition of a correct set of APIs (as explained in
Section~\ref{sec:API}) is crucial to the success of the project,
because a wrong interpretation of the customer's needs will have a
dramatic impact on time, efficiency, costs, and frustration of the IT
team.  This process is called Requirement Analysis and can be defined
as \textit{``the software engineering practice that, at the top level
  of the software architecture, translates stakeholder needs and
  expectations into a viable set of software
  requirements.''}\cite{schmidt2013software}

The best tool to design and present the APIs is the Swagger
Editor\footnote{ \url{https://swagger.io/tools/swagger-editor/} } with
OpenAPI Specification\footnote{an API description format for REST
  APIs}.  It provides an easy to use interface and a great data
visualization that helps in a process prone to errors.  The file
usually consists in YAML \footnote{recursive acronym for YAML Ain't
  Markup Language What It Is, it's a human-friendly data serialization
  standard} code and the output is shown in Figure~\ref{fig:swagger}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/swagger.jpg}
    \caption{The APIs are readable and well presented }
     \label{fig:swagger}
\end{figure}

The swagger file must specify the type of the HTTP request, the format for both the parameters and the responses and
the HTTP status code.
Once that the swagger file is written and approved by the customer, the implementation of APIs can start.




\section{The APIs}
As explained in the introduction (details in Section~\ref{apigroups}),
the APIs can be divided into two groups, back end and front end.
The task accomplished by the back end group is a full reindex of a product catalog,
so we need an API to create a new index
\footnote{An Elasticsearch index is a collection of documents that are related to each other.}
 on ElasticSearch, another one to add hundreds of products to the freshly created index,
 and finally, an API to stop the process. On top of that, we want an API to monitor all the available
 indexes, one to mark an index as active  (the one that will be used) and one to update existing products.

These APIs must be executed in the right order: first create a new index with /reindex/start,
 then add data to it with /reindex/bulk and finally stop the process with reindex/stop. Changing
 the order or using the same call twice (more consecutive reindex/start for example) will result in an error response.

The front end APIs are conceptually simpler since they don't require to be called in a precise order,
but are harder to implement, since they execute fairly complex queries on the active index.



\section{The back end APIs for Full Reindex}
\label{sec:full}
\subsection{/reindex/start}
\label{sub:start}
This API is for starting the reindex process. No parameters are required,
but a condition must be satisfied: the reindexing process can't be already started.
The function checks with \textit{check\_started()} if the reindexing process was already started, and returns error 400
(details in Section~\ref{tab:error-codes}) if it was. 

The status of the reindexing process is saved in a boolean value in DynamoDB, so this function
checks the status \footnote{Check this value the auxiliary function check\_started()}
 before creating a new index and if it is false then create an index with the UNIX timestamp as a name.

\subsection{/reindex/bulk}
\label{sub:bulk}
As said before, this API aims to load up to 1000 products in the active index. 
This value is hardcoded in a global value in the lambda body, however, it is the maximum value
that will operate safely, avoiding breaking the lambda payload limit. Decreasing it will not make any change
but increasing it will cause troubles, so better leave it as it is.
In technical terms, a \textit{bulk operation} is an action performed on large scale,
processing a huge number of items. 

ElasticSearch offers a Bulk API call that
\textit{"performs multiple indexing or delete operations in a single API call. 
This reduces overhead and can greatly increase indexing speed."}\cite{bulk}
ElasticSearch is a powerful tool, but AWS Lambda has a payload limit of 6MB\label{payload}\footnote{https://docs.aws.amazon.com/lambda/latest/dg/limits.html} 
so for the sake of simplicity and to respect this limit the number of products that can be processed in a single API call 
is set to 1000.

The function require a \textit{body} parameter containg the index name
generated by /reindex/start (details in Section~\ref{sub:start}) and
an array of assets.  The error codes used returned by the API are
explained in Table~\ref{tab:error-codes}.
\begin{table}[t]
  \centering
  \begin{tabular}{rl}
    \hline
    Code & Explanation \\
    \hline
    400 & Invalid Request: data not readable or index does not exists \\
    409 & Invalid state: reindexing not started or started twice \\
    413 & Payload too large: too many assets requested \\
    500 & Internal server error, please retry later \\
    200 & Success! \\
    \hline
  \end{tabular}
  \caption{Error codes of interest.}
  \label{tab:error-codes}
\end{table}
A schematic presentation of this endpoint can be $\rightarrow$
\begin{enumerate}
    \item data is not readable  $\rightarrow$ error 400, check error table~\ref{tab:error-codes}.
    \item provided index doesn't exists  $\rightarrow$ error 400 check error table~\ref{tab:error-codes} 
    \item reindexing process was not started or index is not the valid target  $\rightarrow$  error 409, check error table~\ref{tab:error-codes}
    \item  !(number of assets  \textless 1000 \&\& \textless size 6MB) $\rightarrow$ 413, check error table~\ref{tab:error-codes}
    \item now can add assets 
    \item return code 200, check table~\ref{tab:error-codes}
    \item if any exception return error 500, check table~\ref{tab:error-codes}
    
\end{enumerate}

\subsection{/reindex/stop}
\label{sub:stop}
This endpoint requires the name of the index on which the reindexing was performed.
Obviously the function checks that the index provided is the same generated by /reindex/start (details in Section~\ref{sub:start}),
but since ElasticSearch allows the use of aliases for indexes' names, it also checks that 
provided alias is equal to the alias in use and that the alias in use points to the index
generated by /reindex/start (details in Section~\ref{sub:start}).

If these checks are successful, the reindexing status is updated in DynamoDB and the function returns
a confirmation message with the number of assets present in the index at the moment.

\subsection{/reindex/status}
\label{sub:status}
This simple function retrieves information from ElasticSearch about all the available indexes,
and returns the  \textit{active\_index} and  the array \textit{available\_indices} 
with their respective names, number of assets and creation date. 

The simplicity of this endpoint is made possible by using different auxiliary components 
such as \textit{get\_active\_index()}, a simple function that returns the active index.


\subsection{/reindex/promote}
\label{sub:promote}
This endpoint will set the provided index as active, so it will become
queryable by the clients.
The returned values of this function are the same of /reindex/status (details in Section~\ref{sub:status}), because 
after promoting the index it calls the status function to give both a confirmation message
and an understanding of the situation.

\subsection{/delta/update}
\label{sub:update}
This endopoint updates the specified index with the provided items and
each asset is mapped with its SKU number\footnote{SKU is the acronym for Stock-Keeping Unit} as key.
This endpoint has the same limits of the bulk function (details in Section~\ref{sub:bulk}), so a maximum of 1000 assets and 6MB payload, and if they
are not respected the function returns error 413, check the error in Table~\ref{tab:error-codes}.

Similarly, if the input data is not readable an error 400 (check Table\ref{tab:error-codes}) is thrown, but the similarities end here since the 
function loads the items one by one in ElasticSearch calling \textit{es\_client.save()}, an auxiliary function
in the generic ElasticSearch client that either creates or updates an existing item.


\section{The front end APIs}
\label{sec:front_end}

These APIs are going to be called by the clients, as shown in \ref{fig:architecture} and 
offer different endpoints to retrieve data.

\label{def:pagination}An important concept to explain is pagination, since it's frequently used in these APIs due to 
already mentioned (details in Section~\ref{payload}) limits of Lambda functions.
Pagination is \textit{the process of dividing a document into discrete pages, either electronic pages or printed pages}
\footnote{https://en.wikipedia.org/wiki/Pagination} and it's implemented in every front end endpoint of this project.
Solving this problem when working with lambdas is crucial and even very simple tasks can become challenging due to the
implementation of pagination.

In order to implement pagination, two parameters are required and extensively used in the code: \textit{from} and
\textit{size}. The former identifies the first element returned in a ordered list and the latter 
the number of elements returned in a single lambda call.


\subsection{/getAllSKUNumbers}
\label{sub:getallskunumbers}
This endpoint requires a boolean parameter \textit{allSKUs} an optional \textit{from} and
\textit{size} with default values respectively 0 and 5000. 
 The value 5000 is hardcoded but it is a reasonable amount since the values returned are integers, it's better to use the default value to avoid
 hitting the lambda's payload limit.
 If \textit{allSKUs} is set at True returns all the available SKU Numbers, else if set to False returns just the products with
the availability group array empty.

For clarification, the \textit{availability group} is an array where are stored 
the country in which the product is available, for example
a product X will have  \textit{availability group}=["GB","RU","FR"].
The specification for this endpoint is arguable, because it's not a clean 
design to have an API called \textit{getAllSKUNumbers} and then ask  
if you want to get all SKU Numbers, but that's what the customer wanted.

The endpoint returns  the 
\textit{total\_sku\_numbers} 
and the array \textit{all\_sku\_numbers}

\subsection{/getSKUdetails}
\label{sub:getskudetails}
This API returns an array with detailed product information given an
array of product IDs.  The input parameter is \textit{sku\_numbers }
and the data returned is \textit{skus}, in which every product is
mapped with its ID as a key.  Pagination is not required because there
is a limit of 100 products.

If this limit is not respected, the endpoint will return error 413
(check Table~\ref{tab:error-codes}) and if one or more IDs can't be
found in ElasticSearch the result is an error message "SKU not found"
with the SKU that wasn't found and the classic code 404,check the
error Table~\ref{tab:error-codes}.

\subsection{/getAllSKU}
\label{sec:getallsku}
The input parameters  are the  \textit{availability\_group} array (an array of string called “availabilities“ in the product JSON) and 
\textit{language\ array}, another array of strings that defaults to en-EN. 
Returns just products containing at least one of the availability groups passed as input in the array called 'availabilities', grouped per availability group.

Let's say we have a product X and a product Y. X has three availability zones (countries in which the product is available)
GB, RU and DE, while product Y has GB, RU and FR. 
If we request   availability\_group= "GB, FR", ignoring the language for now, we have a result like
[X, Y, Y]. 

The real data format returned with status 200 (check Table~\ref{tab:error-codes}) is
\begin{lstlisting}
    {
    "total_items": "string",
    "from": "string",
    "size": "string",
    "skus": [
        "0001": {product one info},
        "0002": {} 
    ]
    }    
\end{lstlisting}

After these considerations, we must reintroduce the \textit{language\_array},  its fallback mechanism, and localizable fields.
The localizable fields are pieces of information inside a product JSON that are available in several languages. 
This is the algorithm as requested by the customer:

\begin{itemize}
    \item check if the language parameters are valid
    
    \item if none of the input languages is valid(contained in the language table) do not return any product.

    \item If at least one language is valid, use the language table in the following way
    \begin{itemize}
        \item For all the fields to be localized 
        \item show only the values that are matching the languages passed
    as parameter
    \item for each value:
        \begin{itemize}
            \item if one of the languages passed as a parameter does not
            match any value
            \begin{itemize}
                \item try with a fallback language (some languages have
                        fallback languages)
                \item The localized values should be always included in a
                JSON object representing the matched language even
                in case of multiple languages provided as input
                input: "Es,LV" Es: "Espagnol", LV: ""


                
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

If languages or their fallbacks do not match any localized value, these values won't be localized.
The language table is very long (more than 300 lines) and pasting it here wouldn't be helpful, let's just say that
for every language, there is an array of fall-back languages that can be occasionally empty, and to
make it usable in this python project I loaded it in a dictionary using a script.

Like other APIs, getAllSKu implements pagination (details in Section~\ref{def:pagination}) using the parameter \textit{next\_token}.
To properly implement pagination, we divide the results into a set of finite pages and then assign to each one a 
token ID. When calling the API for the first time we just provide the parameter for the query and get as a result
the first page and \textit{next\_token}. From the second call to the last, we just provide the \textit{next\_token}, following the
chain of tokens until the last page.

When started, the function establishes the connection to DynamoDB using Boto3 (details in Section~\ref{def:boto3}) methods.
After a successful connection to DynamoDB, the function checks if the input is valid and differentiate between two
situations: 
\begin{itemize}
    \item The first call, so the client provided the parameters for the query ()
    \item  2nd to n-th calls: the client provided  \textit{next\_token}, so the query was already executed and the function must retrieve the result from DynamoDB.
\end{itemize}

Let's start with the first case. 
The idea is:
\begin{enumerate}
    \item Load all results from the elastisearch query in \textit{all\_ids\_zone} and calculate \textit{total\_items}. 
    \item write\textit{asset\_array\_to\_dynamo} to DynamoDB, in blocks of lenght = \textit{pagination\_size},giving each of them an unique token id= request\_id+from.
    \item  return first asset array and \textit{next\_token} for pagination 
\end{enumerate}

To complete step 1 we must understand how many results the query will return and to accomplish this
we need an auxiliary function \textit{get\_total\_hits} that will perform the same query, but returns just 
the number of total hits, so it's very light and quick.
With this number, we can calculate how many iterations we need to extract all the results 
while respecting the pagination limit, that still applies since we are accessing
ElasticSearch with a lambda function.

With a for cycle, we extract all the products matching the query and put them inside an array.

The second step is  a nice for cycle over a number calculated in a very un-pythonic
\footnote{https://docs.python-guide.org/writing/style/} fashion
\begin{lstlisting}
    for repetition in range(int(math.ceil(len(all_ids_zone)/pagination_size))) 
\end{lstlisting}

 We have a huge array with all the product IDs, so we need to split it in small arrays that
respects the pagination limit, by extracting the elements in each iteration
from \textit{dynamo\_from} (which is 0 in the firt iteration) to pagination\_size*(repetition+1).
Then each of the small arrays must be sent to DynamoDB with its token as ID, and 
the time to live set to half an hour to prevent DynamoDB to became overpopulated with useless data.
After, \textit{dynamo\_from} is set to (repetition+1)*pagination\_size and the cicle starts again.

In step 3 we have a DynamoDB table full of entries with unique keys containing
groups of product IDs (SKUs) grouped by language availability. These groupings are not precisely
contained in a single entry but are shared across multiple ones.
Now we need to return the first page of results, but before doing it 
we must localize the language for each product.
To do it, we need the auxiliary function
\textit{localize\_fields(asset, request\_languages)}, which takes one product and
a list of languages to localize in about 50 different properties.
These fields are generally in the form \textit{property-X/property-Y/property-Z}, they are already defined
in the requirements specified by the customer and are stored in \textit{field\_to\_localize}.

For each ID in the array in DynamoDB, the function retrieves the complete product from ElasticSearch,
runs \textit{localize\_fields} on it, adds it to an array and finally returns the array.
